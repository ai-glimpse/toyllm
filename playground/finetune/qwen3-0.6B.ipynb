{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f14b1293-0b0c-4774-b931-058e3394175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ce1d3-a306-4ce2-8b23-373d3326f46c",
   "metadata": {},
   "source": [
    "## Raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3d8fbc-a57b-4fcd-92f3-c8a1bc19ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ce7f64-bb5d-4eec-8a51-9b2bdb183186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: å½“ç„¶å¯ä»¥ï¼ä½ ä½œä¸ºç¼–ç¨‹å°ç™½ï¼Œå­¦Pythonæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ï¼Œå°¤å…¶å¦‚æœä½ æƒ³**å…ˆå­¦Webå¼€å‘**ï¼Œè¿™ä¼šæ˜¯ä¸€ä¸ªéå¸¸æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›**æ¨èçš„å­¦ä¹ èµ„æºã€æ•™ç¨‹å’Œå­¦ä¹ è·¯çº¿**ï¼Œå¸Œæœ›èƒ½å¸®åŠ©ä½ é¡ºåˆ©å…¥é—¨å¹¶æå‡ï¼š\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ§  **æ¨èå­¦ä¹ èµ„æºï¼š**\n",
      "\n",
      "#### 1. **å…è´¹å­¦ä¹ èµ„æº**\n",
      "- **Codecademy**ï¼ˆé€‚åˆåˆå­¦è€…ï¼‰  \n",
      "  - [Pythonå…¥é—¨æ•™ç¨‹](https://www.codecademy.com/learn/python)  \n",
      "  - [Webå¼€å‘å…¥é—¨](https://www.codecademy.com/learn/web)\n",
      "\n",
      "- **Khan Academy**ï¼ˆé€‚åˆç³»ç»Ÿå­¦ä¹ ï¼‰  \n",
      "  - [Python](https://www.khanacademy.org)  \n",
      "  - [Webå¼€å‘](https://www.khanacademy.org)\n",
      "\n",
      "- **Udemy**ï¼ˆé€‚åˆç³»ç»Ÿå­¦ä¹ ï¼‰  \n",
      "  - [Pythonç¼–ç¨‹å…¥é—¨](https://www.udemy.com/course/learn-python/)  \n",
      "  - [Webå¼€å‘å…¥é—¨](https://www.udemy.com/course/learn-web-development/)\n",
      "\n",
      "- **CodePen**ï¼ˆåœ¨çº¿é¡¹ç›®å®è·µï¼‰  \n",
      "  - [Pythoné¡¹ç›®å®æˆ˜](https://codepen.io/)  \n",
      "  - [Webå¼€å‘é¡¹ç›®å®æˆ˜](https://codepen.io/)\n",
      "\n",
      "- **LeetCode**ï¼ˆç¼–ç¨‹æŒ‘æˆ˜ï¼‰  \n",
      "  - [Pythonå…¥é—¨](https://leetcode.com/learn/learn-python/)  \n",
      "  - [Webå¼€å‘å…¥é—¨](https://leetcode.com/learn/web)\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. **æ¨èæ•™ç¨‹ï¼ˆå…¨æ ˆWebæ–¹å‘ï¼‰**\n",
      "- **W3Schools**  \n",
      "  - [Python](https://www.w3schools.com/Python/)  \n",
      "  - [JavaScript](https://www.w3schools.com/)\n",
      "\n",
      "- **æ…•è¯¾ç½‘ï¼ˆMOOCï¼‰**  \n",
      "  - [Python](https://www.imooc.com/learn/1021690.html)  \n",
      "  - [Webå¼€å‘](https://www.imooc.com/learn/1021690.html)\n",
      "\n",
      "- **çŸ¥ä¹ä¸“æ **  \n",
      "  - [Pythonå…¥é—¨](https://www.zhihu.com/question/450426811)  \n",
      "  - [Webå¼€å‘å…¥é—¨](https://www.zhihu.com/question/450426811)\n",
      "\n",
      "---\n",
      "\n",
      "#### 3. **æ¨èå­¦ä¹ è·¯å¾„ï¼ˆWebå¼€å‘ï¼‰ï¼šä»åŸºç¡€åˆ°è¿›é˜¶**\n",
      "\n",
      "- **åŸºç¡€é˜¶æ®µï¼ˆ1-2ä¸ªæœˆï¼‰ï¼š**\n",
      "  - å­¦PythonåŸºç¡€è¯­æ³•ã€æ•°æ®ç»“æ„ã€å¾ªç¯ã€æ¡ä»¶è¯­å¥ç­‰ã€‚\n",
      "  - å­¦HTML/CSSåŸºç¡€ï¼Œå¯ä»¥å…ˆä»[HTML5/CSS3](https://www.w3schools.com/html/html5.asp)å¼€å§‹ã€‚\n",
      "  - å¯ä»¥å°è¯•**CodePen**æˆ–**JSFiddle**æ¥å®è·µã€‚\n",
      "\n",
      "- **è¿›é˜¶é˜¶æ®µï¼ˆ3-6ä¸ªæœˆï¼‰ï¼š**\n",
      "  - å­¦å‰ç«¯æ¡†æ¶ï¼ˆå¦‚Reactã€Vue.jsï¼‰ã€‚\n",
      "  - å­¦åç«¯å¼€å‘ï¼ˆå¦‚Node.jsã€Python Flaskï¼‰ã€‚\n",
      "  - å¯ä»¥å°è¯•**Git**å’Œ**ç‰ˆæœ¬æ§åˆ¶**ï¼Œå­¦ä¹ å¦‚ä½•ç®¡ç†ä»£ç ã€‚\n",
      "\n",
      "- **å®æˆ˜é˜¶æ®µï¼ˆ6-12ä¸ªæœˆï¼‰ï¼š**\n",
      "  - æ­å»ºä¸€ä¸ªç®€å•çš„ç½‘ç«™ï¼ˆå¦‚åšå®¢ã€ä¸ªäººé¡¹ç›®ï¼‰ã€‚\n",
      "  - å‚ä¸å¼€æºé¡¹ç›®æˆ–GitHubé¡¹ç›®ï¼Œæå‡å®æˆ˜èƒ½åŠ›ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ’¡ **å­¦ä¹ å»ºè®®ï¼š**\n",
      "\n",
      "1. **å…ˆä»Pythonå¼€å§‹ï¼Œå†é€æ­¥å­¦ä¹ Webå¼€å‘ã€‚**\n",
      "2. **æ¯å¤©åšæŒå­¦ä¸€å°æ—¶ï¼Œå“ªæ€•åªæ˜¯å†™ä¸€ä¸ªç®€å•çš„ä»£ç ã€‚**\n",
      "3. **ä¸è¦æ€¥äºæ±‚æˆï¼Œé‡åˆ°å›°éš¾æ—¶å…ˆçœ‹æ•™ç¨‹ï¼Œæ…¢æ…¢ç§¯ç´¯ç»éªŒã€‚**\n",
      "4. **å…³æ³¨ç¤¾åŒºï¼ˆå¦‚Stack Overflowã€Redditã€Discordï¼‰**ï¼Œå’Œå­¦ä¹ è€…äº¤æµï¼Œæ‰¾åˆ°é€‚åˆä½ çš„å­¦ä¹ æ–¹å¼ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "å¦‚æœä½ æœ‰æ—¶é—´ï¼Œä¹Ÿå¯ä»¥**å…ˆå­¦å‰ç«¯ï¼ˆHTML/CSS/JSï¼‰**ï¼Œç„¶åå†å­¦ä¹ Pythonå’ŒWebå¼€å‘ã€‚å¸Œæœ›ä½ èƒ½é¡ºåˆ©å…¥é—¨ï¼åŠ æ²¹ï¼ ğŸš€\n"
     ]
    }
   ],
   "source": [
    "# prepare the model input\n",
    "# prompt = \"\"\"Give me the summary of the text: A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
    "# The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]\n",
    "# \"\"\"\n",
    "prompt = \"\"\"æˆ‘æ˜¯ä¸€ä¸ªç¨‹åºå°ç™½ï¼Œä¸€ç›´æƒ³å­¦å¥½å­¦ç²¾ä¸€é—¨ç¼–ç¨‹ã€‚çœ‹ç½‘ä¸Šè¯´ python å¥½å­¦ï¼Œå°±æ‰“ç®—å†³å®šå­¦ python ï¼Œç„¶åå°±åœ¨ç½‘ä¸Šæ‰¾è§†é¢‘å­¦ä¹ ï¼Œå…ˆå­¦ä¹ äº†ä¸€ä¸ªå°šç¡…è°·å“ªä¸ª python ï¼Œæˆ‘æ„Ÿè§‰è¿˜å¯ä»¥ï¼Œå°±è®¤è®¤çœŸçœŸçš„å­¦äº†ã€‚ä¸€äº›åŸºç¡€ä¸œè¥¿ç®—æ˜¯ä¼šäº†å§ã€‚æƒ³æé«˜ã€‚å¤§è‡´çš„æ–¹å‘æ˜¯å…ˆ web æ–¹å‘ï¼Œå› ä¸ºæƒ³è‡ªå·±æ­å»ºä¸€ä¸ªç½‘ç«™æ˜¯ä¸€ä¸ªåˆšéœ€ã€‚ä½†åœ¨ç½‘ä¸Šæ‰¾ä¸åˆ°å¥½çš„å­¦ä¹ èµ„æºäº†ï¼Œä¹Ÿä¸çŸ¥é“ä»å“ªé‡Œåˆ°å“ªé‡Œå…ˆå­¦äº†ã€‚ å‰æ®µæ—¶é—´çœ‹è¦å…ˆå­¦å‰ç«¯ï¼Œå­¦å…¨æ ˆï¼Œåœ¨ bili ä¸Šæœäº†ä¸€åœˆä¹Ÿæ‰¾ä¸åˆ°ä¸€ä¸ªåˆé€‚çš„æ•™ç¨‹ã€‚ ç°åœ¨æˆ‘æƒ³è¯·å„ä½å¤§å“¥ï¼Œå¤§å§ï¼Œå¼Ÿå¼Ÿï¼Œå¦¹å¦¹ä»¬ï¼Œå¤§ç¥ä»¬ç»™ä¿ºæ¨èä¸€ä¸ªå¥½çš„å­¦ä¹ æ•™ç¨‹æˆ–è€…å­¦ä¹ æ–¹æ³•åŠè·¯çº¿ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼Œæ•™ç¨‹ç»™æ¨èå¥½çš„å…è´¹çš„æœ€å¥½ï¼\n",
    "\\n\\nTL;DR:\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,  # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :].tolist()\n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ccc05-1e12-4ebb-9c35-a56219260aea",
   "metadata": {},
   "source": [
    "## Peft model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3ed838-78d1-43fd-829a-7ce4cb1f3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae1abe7-b450-4527-8d34-845a6ece7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,046,272 || all params: 601,096,192 || trainable%: 0.8395\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33926c49-e778-4f90-aab4-3756970d6ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0179f3-d022-414a-b1b5-500675e02c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a237ab9-5f84-4a4c-84c3-a5a424afe964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f85f04-ff93-4182-bae6-43298fbdb0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"trl-lib/tldr\", split=\"train[:20000]\")\n",
    "\n",
    "# dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3876b1-0454-4b6f-a992-e579422865b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
    "# Converts dataset from prompt/completion format (not supported anymore)\n",
    "# to the conversational format\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {\"messages\": output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {\"messages\": converted_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ebe363-5ecc-4231-906a-3926586a0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_dataset).remove_columns([\"prompt\", \"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fea1168-951d-4606-8b5f-3bbb296900d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': \"SUBREDDIT: r/relationships\\n\\nTITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\\n\\nPOST: Not sure if this belongs here but it's worth a try. \\n\\nBackstory:\\nWhen I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \\n\\nNow: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \\n\\nHis friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \\n\\nSo I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\\n\\nTL;DR:\",\n",
       "   'role': 'user'},\n",
       "  {'content': \" I still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1c39b-2839-4478-9fc6-cbc1daa62008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ceea90a-47b8-464e-8ee9-e365e07e2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,\n",
    "    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=1,\n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=1024 + 512,\n",
    "    # If batch size would cause OOM, halves its size until it works\n",
    "    auto_find_batch_size=True,\n",
    "    ## GROUP 2: Dataset-related\n",
    "    max_seq_length=64,\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=True,\n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    # Optimizer\n",
    "    # optim='adamw',\n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    output_dir=\"./qwen3-0.6B-tldr-adapter\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24be10b5-4d15-4f78-9af2-68cea91c90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3098a2d4-f7ab-4562-a79a-5ceed9e6789a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmathewshen\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mathewshen/Workspace/Projects/github/toyllm/playground/finetune/wandb/run-20250513_220140-xqdk2ol2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mathewshen/huggingface/runs/xqdk2ol2' target=\"_blank\">./qwen3-0.6B-tldr-adapter</a></strong> to <a href='https://wandb.ai/mathewshen/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mathewshen/huggingface' target=\"_blank\">https://wandb.ai/mathewshen/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mathewshen/huggingface/runs/xqdk2ol2' target=\"_blank\">https://wandb.ai/mathewshen/huggingface/runs/xqdk2ol2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1243' max='1243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1243/1243 55:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.990900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.987200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.950400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.943500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.947200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.939900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.948700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.932600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.925800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.944300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.928500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.944800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.912200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.927200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.937300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.909200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.894100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.934300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.908300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.907700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.904100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.910500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.909100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.920800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.889900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.901700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.898600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1243, training_loss=2.94467625575783, metrics={'train_runtime': 3310.4652, 'train_samples_per_second': 36.029, 'train_steps_per_second': 0.375, 'total_flos': 2.0404899409821696e+16, 'train_loss': 2.94467625575783})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3d398-9a62-428e-8df9-3da02af30e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e37cdee-355e-43c7-a367-1287777de12a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  ä¸€ä¸ªç¨‹åºå°ç™½ï¼Œæƒ³å­¦å¥½å­¦ç²¾ä¸€é—¨ç¼–ç¨‹ï¼Œæƒ³å…ˆå­¦ python ï¼Œç„¶åå­¦ web ï¼Œç„¶åå­¦å‰ç«¯ï¼Œæœ€åå­¦å…¨æ ˆã€‚æƒ³æé«˜ã€‚æƒ³è¯·å„ä½å¤§å“¥ï¼Œå¤§å§ï¼Œå¼Ÿå¼Ÿï¼Œå¦¹å¦¹ä»¬ï¼Œå¤§ç¥ä»¬ç»™ä¿ºæ¨èå¥½çš„å­¦ä¹ æ•™ç¨‹æˆ–å­¦ä¹ æ–¹æ³•åŠè·¯çº¿ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼Œæ•™ç¨‹ç»™æ¨èå¥½çš„å…è´¹çš„æœ€å¥½ï¼\n"
     ]
    }
   ],
   "source": [
    "# prepare the model input\n",
    "# prompt = \"\"\"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
    "# The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]\n",
    "# \\n\\nTL;DR:\n",
    "# \"\"\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"./qwen3-0.6B-tldr-adapter/checkpoint-1243/\").to(model.device)\n",
    "\n",
    "\n",
    "prompt = \"\"\"æˆ‘æ˜¯ä¸€ä¸ªç¨‹åºå°ç™½ï¼Œä¸€ç›´æƒ³å­¦å¥½å­¦ç²¾ä¸€é—¨ç¼–ç¨‹ã€‚çœ‹ç½‘ä¸Šè¯´ python å¥½å­¦ï¼Œå°±æ‰“ç®—å†³å®šå­¦ python ï¼Œç„¶åå°±åœ¨ç½‘ä¸Šæ‰¾è§†é¢‘å­¦ä¹ ï¼Œå…ˆå­¦ä¹ äº†ä¸€ä¸ªå°šç¡…è°·å“ªä¸ª python ï¼Œæˆ‘æ„Ÿè§‰è¿˜å¯ä»¥ï¼Œå°±è®¤è®¤çœŸçœŸçš„å­¦äº†ã€‚ä¸€äº›åŸºç¡€ä¸œè¥¿ç®—æ˜¯ä¼šäº†å§ã€‚æƒ³æé«˜ã€‚å¤§è‡´çš„æ–¹å‘æ˜¯å…ˆ web æ–¹å‘ï¼Œå› ä¸ºæƒ³è‡ªå·±æ­å»ºä¸€ä¸ªç½‘ç«™æ˜¯ä¸€ä¸ªåˆšéœ€ã€‚ä½†åœ¨ç½‘ä¸Šæ‰¾ä¸åˆ°å¥½çš„å­¦ä¹ èµ„æºäº†ï¼Œä¹Ÿä¸çŸ¥é“ä»å“ªé‡Œåˆ°å“ªé‡Œå…ˆå­¦äº†ã€‚ å‰æ®µæ—¶é—´çœ‹è¦å…ˆå­¦å‰ç«¯ï¼Œå­¦å…¨æ ˆï¼Œåœ¨ bili ä¸Šæœäº†ä¸€åœˆä¹Ÿæ‰¾ä¸åˆ°ä¸€ä¸ªåˆé€‚çš„æ•™ç¨‹ã€‚ ç°åœ¨æˆ‘æƒ³è¯·å„ä½å¤§å“¥ï¼Œå¤§å§ï¼Œå¼Ÿå¼Ÿï¼Œå¦¹å¦¹ä»¬ï¼Œå¤§ç¥ä»¬ç»™ä¿ºæ¨èä¸€ä¸ªå¥½çš„å­¦ä¹ æ•™ç¨‹æˆ–è€…å­¦ä¹ æ–¹æ³•åŠè·¯çº¿ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼Œæ•™ç¨‹ç»™æ¨èå¥½çš„å…è´¹çš„æœ€å¥½ï¼\n",
    "\\n\\nTL;DR:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,  # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=1e-6,\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :].tolist()\n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afbb9b8e-93d8-4c65-bec5-1dd783ca98b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2609f3ab614da68d842f04fe05ef28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcf904d1ce4447893f0d552457d83d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/20.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a98dabc2e14b04ac6d8854215a37d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/6.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MathewShen/qwen3-0.6B-tldr-adapter/commit/74d9c95bc90999ed61d58ea9603295b98c7d364a', commit_message='End of training', commit_description='', oid='74d9c95bc90999ed61d58ea9603295b98c7d364a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/MathewShen/qwen3-0.6B-tldr-adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='MathewShen/qwen3-0.6B-tldr-adapter'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26991675-9c13-47ac-9ffd-57898cf38cf0",
   "metadata": {},
   "source": [
    "## Use peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39cc2829-de7d-4d89-b3e5-76a5ec5ddef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"MathewShen/qwen3-0.6B-tldr-adapter\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f36d1e6b-e10f-4b7b-9aaf-0980a10f9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"\"\"æˆ‘æ˜¯ä¸€ä¸ªç¨‹åºå°ç™½ï¼Œä¸€ç›´æƒ³å­¦å¥½å­¦ç²¾ä¸€é—¨ç¼–ç¨‹ã€‚çœ‹ç½‘ä¸Šè¯´ python å¥½å­¦ï¼Œå°±æ‰“ç®—å†³å®šå­¦ python ï¼Œç„¶åå°±åœ¨ç½‘ä¸Šæ‰¾è§†é¢‘å­¦ä¹ ï¼Œå…ˆå­¦ä¹ äº†ä¸€ä¸ªå°šç¡…è°·å“ªä¸ª python ï¼Œæˆ‘æ„Ÿè§‰è¿˜å¯ä»¥ï¼Œå°±è®¤è®¤çœŸçœŸçš„å­¦äº†ã€‚ä¸€äº›åŸºç¡€ä¸œè¥¿ç®—æ˜¯ä¼šäº†å§ã€‚æƒ³æé«˜ã€‚å¤§è‡´çš„æ–¹å‘æ˜¯å…ˆ web æ–¹å‘ï¼Œå› ä¸ºæƒ³è‡ªå·±æ­å»ºä¸€ä¸ªç½‘ç«™æ˜¯ä¸€ä¸ªåˆšéœ€ã€‚ä½†åœ¨ç½‘ä¸Šæ‰¾ä¸åˆ°å¥½çš„å­¦ä¹ èµ„æºäº†ï¼Œä¹Ÿä¸çŸ¥é“ä»å“ªé‡Œåˆ°å“ªé‡Œå…ˆå­¦äº†ã€‚ å‰æ®µæ—¶é—´çœ‹è¦å…ˆå­¦å‰ç«¯ï¼Œå­¦å…¨æ ˆï¼Œåœ¨ bili ä¸Šæœäº†ä¸€åœˆä¹Ÿæ‰¾ä¸åˆ°ä¸€ä¸ªåˆé€‚çš„æ•™ç¨‹ã€‚ ç°åœ¨æˆ‘æƒ³è¯·å„ä½å¤§å“¥ï¼Œå¤§å§ï¼Œå¼Ÿå¼Ÿï¼Œå¦¹å¦¹ä»¬ï¼Œå¤§ç¥ä»¬ç»™ä¿ºæ¨èä¸€ä¸ªå¥½çš„å­¦ä¹ æ•™ç¨‹æˆ–è€…å­¦ä¹ æ–¹æ³•åŠè·¯çº¿ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼Œæ•™ç¨‹ç»™æ¨èå¥½çš„å…è´¹çš„æœ€å¥½ï¼\n",
    "# \\n\\nTL;DR:\n",
    "# \"\"\"\n",
    "\n",
    "question = \"\"\"æœ€è¿‘åœ¨å­¦ linux å’Œ docker,å¥½å¤šå­¦ä¹ èµ„æ–™éƒ½æ˜¯é»˜è®¤åœ¨ linux ç¯å¢ƒï¼Œä»¥åä¹Ÿæƒ³å¾€åè¿ç»´æ–¹å‘è½¬ï¼Œæƒ³å¼„å°ä¾¿å®œè½»ç‚¹çš„ç¬”è®°æœ¬å­¦ linux ï¼Œç›®å‰æœ‰ä¸ªç¬”è®°æœ¬åœ¨å®¶å½“å°å¼æœºç”¨äº†ï¼Œè™šæ‹Ÿæœºæ€»æ„Ÿè§‰ä¸å¤ŸæŠ•å…¥ï¼Œå¤§å®¶æœ‰å•¥æ¨èçš„æœ¬å­ä¸ï¼Œæˆ‘æƒ³è¶Šä¾¿å®œè¶Šå¥½ï¼ˆä¸è¦å¤ªå¡ï¼‰ï¼Œç›®å‰æ„Ÿè§‰ chromebook ä¸é”™ï¼Œè£…è½»é‡çº§ linux å­¦ä¹ ç”¨å¤Ÿäº†ï¼Œå¤§å®¶æœ‰å•¥æ¨èæˆ–è€…æ€è·¯ä¸.\n",
    "\\n\\nTL;DR:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e02026f-d825-4dfa-b911-3f453814e1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      " æœ¬å­é¢„ç®—æœ‰é™ï¼Œæƒ³å­¦ linux å’Œ dockerï¼Œæƒ³å¼„å°ä¾¿å®œè½»ç‚¹çš„ç¬”è®°æœ¬å­¦ linuxï¼Œç›®å‰æ„Ÿè§‰ chromebook ä¸é”™ï¼Œæƒ³è¶Šä¾¿å®œè¶Šå¥½ã€‚\n"
     ]
    }
   ],
   "source": [
    "output = generator(\n",
    "    [{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False, temperature=0.001\n",
    ")[0]\n",
    "print(output[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca91ee-a2e2-4f8f-9ced-11aa408d8a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9572c30-2187-462c-9dc4-2553b5a3d605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
