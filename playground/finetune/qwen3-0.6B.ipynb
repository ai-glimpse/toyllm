{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14b1293-0b0c-4774-b931-058e3394175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd3d8fbc-a57b-4fcd-92f3-c8a1bc19ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07ce7f64-bb5d-4eec-8a51-9b2bdb183186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: 当然可以！以下是一些**推荐的学习资源和路线**，适合你从**Python基础到Web开发**的全栈学习，同时注重**免费且高效**的教程：\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ 一、推荐学习资源\n",
      "\n",
      "#### 1. **免费 Python 教程**\n",
      "- **Codecademy**（https://www.codecademy.com）  \n",
      "  - 简单易学，适合初学者。\n",
      "- **Udemy**（https://www.udemy.com）  \n",
      "  - 有系统课程，适合全栈开发。\n",
      "- **Coursera**（https://www.coursera.org）  \n",
      "  - 提供专业课程，适合深入学习。\n",
      "- **LeetCode**（https://leetcode.com）  \n",
      "  - 适合练习编程，但需要一定时间。\n",
      "\n",
      "#### 2. **免费 Web 开发教程**\n",
      "- **W3Schools**（https://www.w3schools.com/）  \n",
      "  - 简单易懂，适合入门。\n",
      "- **MDN Web Docs**（https://developer.mozilla.org）  \n",
      "  - 详细且适合学习。\n",
      "- **Stack Overflow**（https://stackoverflow.com）  \n",
      "  - 问答社区，适合找资源和讨论。\n",
      "\n",
      "#### 3. **免费学习平台**\n",
      "- **CodePen**（https://codepen.io）  \n",
      "  - 可以在浏览器中直接运行代码。\n",
      "- **JSFiddle**（https://jsfiddle.net）  \n",
      "  - 可以用来练习基础语法和功能。\n",
      "- **GitHub**（https://github.com）  \n",
      "  - 可以学习项目代码，但需要一定时间。\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ 二、推荐学习路线\n",
      "\n",
      "#### 1. **基础 Python 学习**\n",
      "- **从基础开始**：学变量、循环、条件语句、函数等。\n",
      "- **推荐书籍**：\n",
      "  - 《Python编程：从入门到实践》（作者：John K. Rees）  \n",
      "    - 适合初学者，系统性强。\n",
      "  - 《Python Crash Course》（作者：Ray Kurzweil）  \n",
      "    - 简单易学，适合入门。\n",
      "\n",
      "#### 2. **Web 开发学习**\n",
      "- **前端学习**：\n",
      "  - **HTML/CSS**：学习基础结构和样式。\n",
      "  - **JavaScript**：掌握动态内容、事件处理等。\n",
      "- **后端学习**：\n",
      "  - **Python + Flask**：学如何用 Python 编写 API，搭建网站。\n",
      "  - **Python + Django**：学习 Python 与 Django 的结合。\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ 三、学习建议\n",
      "\n",
      "1. **分阶段学习**：\n",
      "   - 前期：Python基础 + 前端（HTML/CSS/JavaScript）\n",
      "   - 中期：Web全栈（Python + 前端 + 后端）\n",
      "   - 后期：深入学习 Python 优化、分布式系统、机器学习等。\n",
      "\n",
      "2. **结合项目练习**：\n",
      "   - 从简单项目开始，逐步提升难度。\n",
      "   - 参与开源项目或社区项目，提升实战能力。\n",
      "\n",
      "3. **关注官方文档和社区**：\n",
      "   - Python官方文档（https://docs.python.org/3/）\n",
      "   - Stack Overflow、GitHub、Reddit（如 r/learnpython）\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ 四、总结\n",
      "\n",
      "| 资源 | 类型 | 适合人群 |\n",
      "|------|------|----------|\n",
      "| Codecademy | 免费、教程 | 全栈开发者 |\n",
      "| Udemy | 全栈课程 | 有时间 |\n",
      "| Coursera | 专业课程 | 深度学习 |\n",
      "| W3Schools | 简易教程 | 初学者 |\n",
      "| MDN | 详细文档 | 学习者 |\n",
      "| GitHub | 项目练习 | 具体实践者 |\n",
      "\n",
      "---\n",
      "\n",
      "如果你有时间，也可以尝试 **CodePen** 或 **JSFiddle** 来练习基础代码，这样能更快掌握 Python 的语法和开发基础。希望你能顺利学习，实现自己的网站项目！ 😊\n"
     ]
    }
   ],
   "source": [
    "# prepare the model input\n",
    "# prompt = \"\"\"Give me the summary of the text: A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
    "# The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]\n",
    "# \"\"\"\n",
    "prompt = \"\"\"我是一个程序小白，一直想学好学精一门编程。看网上说 python 好学，就打算决定学 python ，然后就在网上找视频学习，先学习了一个尚硅谷哪个 python ，我感觉还可以，就认认真真的学了。一些基础东西算是会了吧。想提高。大致的方向是先 web 方向，因为想自己搭建一个网站是一个刚需。但在网上找不到好的学习资源了，也不知道从哪里到哪里先学了。 前段时间看要先学前端，学全栈，在 bili 上搜了一圈也找不到一个合适的教程。 现在我想请各位大哥，大姐，弟弟，妹妹们，大神们给俺推荐一个好的学习教程或者学习方法及路线，越详细越好，教程给推荐好的免费的最好！\n",
    "\\n\\nTL;DR:\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ccc05-1e12-4ebb-9c35-a56219260aea",
   "metadata": {},
   "source": [
    "## PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c3ed838-78d1-43fd-829a-7ce4cb1f3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "                        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae1abe7-b450-4527-8d34-845a6ece7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,046,272 || all params: 601,096,192 || trainable%: 0.8395\n"
     ]
    }
   ],
   "source": [
    "model_lora = get_peft_model(model, peft_config)\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33926c49-e778-4f90-aab4-3756970d6ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba0179f3-d022-414a-b1b5-500675e02c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"trl-lib/tldr\", split=\"train[:100]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc3876b1-0454-4b6f-a992-e579422865b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
    "# Converts dataset from prompt/completion format (not supported anymore)\n",
    "# to the conversational format\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5ebe363-5ecc-4231-906a-3926586a0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_dataset).remove_columns(['prompt', 'completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fea1168-951d-4606-8b5f-3bbb296900d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': \"SUBREDDIT: r/relationships\\n\\nTITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\\n\\nPOST: Not sure if this belongs here but it's worth a try. \\n\\nBackstory:\\nWhen I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \\n\\nNow: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \\n\\nHis friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \\n\\nSo I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\\n\\nTL;DR:\",\n",
       "   'role': 'user'},\n",
       "  {'content': \" I still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1c39b-2839-4478-9fc6-cbc1daa62008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ceea90a-47b8-464e-8ee9-e365e07e2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,\n",
    "    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=1,\n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=64,\n",
    "    # If batch size would cause OOM, halves its size until it works\n",
    "    auto_find_batch_size=True,\n",
    "    \n",
    "    ## GROUP 2: Dataset-related\n",
    "    max_seq_length=64,\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=True,\n",
    "    \n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    # Optimizer\n",
    "    # optim='adamw',\n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./qwen3-0.6B-tldr-adapter',\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24be10b5-4d15-4f78-9af2-68cea91c90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098a2d4-f7ab-4562-a79a-5ceed9e6789a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 45/200 01:55 < 06:56, 0.37 it/s, Epoch 2.20/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.538600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.902700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37cdee-355e-43c7-a367-1287777de12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "# prompt = \"\"\"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
    "# The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]\n",
    "# \\n\\nTL;DR:\n",
    "# \"\"\"\n",
    "model_lora = model.from_pretrained(\"./qwen3-0.6B-tldr-adapter/checkpoint-200/\").to(model.device)\n",
    "\n",
    "prompt = \"\"\"我是一个程序小白，一直想学好学精一门编程。看网上说 python 好学，就打算决定学 python ，然后就在网上找视频学习，先学习了一个尚硅谷哪个 python ，我感觉还可以，就认认真真的学了。一些基础东西算是会了吧。想提高。大致的方向是先 web 方向，因为想自己搭建一个网站是一个刚需。但在网上找不到好的学习资源了，也不知道从哪里到哪里先学了。 前段时间看要先学前端，学全栈，在 bili 上搜了一圈也找不到一个合适的教程。 现在我想请各位大哥，大姐，弟弟，妹妹们，大神们给俺推荐一个好的学习教程或者学习方法及路线，越详细越好，教程给推荐好的免费的最好！\n",
    "\\n\\nTL;DR:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model_lora.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    temperature=0.0001,\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a82e5-79f8-4a6f-aa4d-7527267710ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
