{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b359fff-9644-4ed9-b484-2897f88e833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd25605-773e-4b10-81cf-989ad428eeec",
   "metadata": {},
   "source": [
    "## MHA's input\n",
    "\n",
    "Multi-Head Attention 的输入$h$是shape等于 $[B, S, D]$ 的 Tensor:\n",
    "- B: batch_size\n",
    "- S: content length\n",
    "- D: embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c15ee29-ba17-44b4-981f-126b62b2a652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, S, D = 10, 20, 256\n",
    "\n",
    "x = torch.rand(B, S, D)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8c136-e4a1-4a36-b521-b720106555f7",
   "metadata": {},
   "source": [
    "## MHA's output\n",
    "\n",
    "Multi-Head Attention 的输出$mha(h)$是shape等于 $[B, S, D']$ 的 Tensor. 很多情况下都会保持$D=D'$, 也就是说这种情况下，数据经过MHA前后的shape是不变的(这样也方便后续加shortcut/skip-connection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d53b80-f69d-4a8c-a7ac-479ff9b9c5eb",
   "metadata": {},
   "source": [
    "## MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5ece35-3c42-4e44-9e5a-9719def962d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int = D,\n",
    "        d_out: int = D,\n",
    "        n_head: int = 8,\n",
    "        ctx_len: int = S,\n",
    "        qkv_bias: bool = False,\n",
    "        dropout_rate: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 确保D'可以被n_head整除，否则不能进行多头分割\n",
    "        if d_out % n_head != 0:\n",
    "            raise ValueError(f\"d_out % n_head = {d_out % n_head} != 0\")\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_out // n_head\n",
    "\n",
    "        self.Wq = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.Wk = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.Wv = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.Wo = nn.Linear(d_out, d_out)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.full((ctx_len, ctx_len), -torch.inf).triu_(1))\n",
    "\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B, S, D = x.shape\n",
    "\n",
    "        # [B, S, D] x [D, D'] -> [B, S, D']\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "\n",
    "        # [B, S, D'] -> [B, S, H, Dh]\n",
    "        q = q.view(B, S, self.n_head, self.head_dim)\n",
    "        k = k.view(B, S, self.n_head, self.head_dim)\n",
    "        v = v.view(B, S, self.n_head, self.head_dim)\n",
    "\n",
    "        # [B, S, H, Dh] -> [B, H, S, Dh]\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # attention matrix: [B, H, S, S]\n",
    "        attn_scores = q @ k.transpose(2, 3) / k.shape[-1] ** 0.5\n",
    "\n",
    "        # attention mask\n",
    "        attn_scores += self.mask[:S, :S]\n",
    "        attn_weight = torch.softmax(attn_scores, -1)\n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "\n",
    "        # [B, H, S, S] x [B, H, S, Dh] -> [B, H, S, Dh]\n",
    "        h = attn_weight @ v\n",
    "        #  [B, H, S, Dh] -> [B, S, H Dh]\n",
    "        h = h.transpose(1, 2)\n",
    "        h = h.contiguous().view(B, S, D)\n",
    "\n",
    "        # output projection\n",
    "        o = self.Wo(h)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444aa031-454c-4adf-a4df-67e3ddca455b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 20, 256]), torch.Size([10, 20, 256]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MHA()\n",
    "o = mha(x)\n",
    "(x.shape, o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90529453-77db-41b6-a99b-7fa4bc49ee2f",
   "metadata": {},
   "source": [
    "## 温馨提示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75117edf-2d67-45a2-bae2-c6a3754cfd70",
   "metadata": {},
   "source": [
    "### contiguous().view V.S. reshape\n",
    "\n",
    "在 PyTorch 中，张量的**内存连续性**直接影响能否直接使用 `.view()` 或 `.reshape()`：\n",
    "\n",
    "- **`.view()` 的限制**  \n",
    "  `.view()` 方法要求张量在内存中是**连续存储**的（逻辑顺序与物理存储顺序一致）。如果张量经过转置（`transpose`）、切片（`slice`）等操作后变为非连续，直接调用 `.view()` 会抛出错误。此时必须显式调用 `.contiguous()` 将张量转换为连续布局，再使用 `.view()`。\n",
    "\n",
    "- **`.reshape()` 的隐式处理**  \n",
    "  `.reshape()` 方法会自动处理非连续张量：\n",
    "  - 若张量**已连续**，`.reshape()` 等价于 `.view()`（无额外开销）。\n",
    "  - 若张量**非连续**，`.reshape()` 会隐式调用 `.contiguous()` 生成连续副本，再调整形状。这会引入**潜在的性能损耗**（内存复制）。\n",
    "\n",
    "\n",
    "看了下LLaMA2/3的模型实现，都是采用的`contiguous().view`, 所以这里我们也保留使用这种形式。\n",
    "\n",
    "References:\n",
    "- [What's the difference between torch.reshape vs. torch.view - PyTorch Forums](https://discuss.pytorch.org/t/whats-the-difference-between-torch-reshape-vs-torch-view/159172)\n",
    "- [torch.reshape — PyTorch 2.6 documentation](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape)\n",
    "- [torch.Tensor.view — PyTorch 2.6 documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5fa1f-daee-406f-8798-27d070cbd8c8",
   "metadata": {},
   "source": [
    "### 另外一种mask的实现\n",
    "\n",
    "\n",
    "这里causal mask的作用是通过相加的的方式实现的，在toyllm中gpt2的MHA是通过`masked_fill_`来实现的，两者都是可以的。\n",
    "不过这里却不能直接将gpt2的实现换成这里相加的方式，因为原始实现中`mask`的实现为：\n",
    "\n",
    "```python\n",
    "self.register_buffer(\"mask\", torch.triu(torch.ones(ctx_len, ctx_len), diagonal=1), persistent=True)\n",
    "# ...\n",
    "attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "# Original mask truncated to the number of tokens and converted to boolean\n",
    "mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "# Use the mask to fill attention scores\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "```\n",
    "\n",
    "这里`persistent=True`会使得在存储`model.pt`时候会将`mask`的值一并存入，之后载入模型会随之一起载入，所以这里无法直接替换`mask`为新的形式。\n",
    "\n",
    "不过我们可以通过忽略`self.mask`, 在推理的时候通过函数内定义`mask`来指定使用新的`mask`：\n",
    "\n",
    "```python\n",
    "attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "mask = torch.triu(torch.full((num_tokens, num_tokens), -torch.inf), diagonal=1).to(attn_scores.device)\n",
    "attn_scores = attn_scores + mask\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49624a9f-9e93-487d-9439-36e9c972e317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
