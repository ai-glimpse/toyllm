{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ToyLLM: Learning LLM from Scratch","text":"<p>A hands-on educational project for understanding and implementing Large Language Models (LLMs) from scratch. This project provides implementations of GPT-2 and related techniques, making it an excellent resource for learning about transformer architectures and modern language models.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#gpt-2-implementation","title":"GPT-2 Implementation","text":"<p>A clean, educational implementation of GPT-2 with type hints, supporting both training and inference.</p>"},{"location":"#speculative-sampling","title":"Speculative Sampling","text":"<p>An implementation of speculative sampling for faster inference, featuring configurable draft models and performance benchmarking.</p>"},{"location":"#kv-cache-optimization","title":"KV Cache Optimization","text":"<p>A memory-efficient GPT-2 implementation with KV cache optimization for handling longer sequences.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or 3.12</li> <li>Git and Git LFS (for model files)</li> <li>UV (recommended package manager)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/ai-glimpse/toyllm.git\ncd toyllm\n</code></pre></p> </li> <li> <p>Set up the environment:    <pre><code># Create and activate virtual environment\nuv venv -p 3.12\nsource .venv/bin/activate\n\n# Install toyllm\nuv pip install toyllm\n</code></pre></p> </li> <li> <p>Download model files:    <pre><code># Install Git LFS if not already installed\ngit lfs install\n\n# Download model files\ngit clone https://huggingface.co/MathewShen/toyllm-gpt2 models\n</code></pre></p> </li> </ol> <p>Alternatively, you can manually download the model files from Hugging Face and place them in the <code>toyllm/models</code> directory.</p>"},{"location":"#usage-examples","title":"Usage Examples","text":""},{"location":"#basic-gpt-2-inference","title":"Basic GPT-2 Inference","text":"<pre><code>python toyllm/cli/run_gpt2.py --help  # View available options\npython toyllm/cli/run_gpt2.py         # Run with default settings\n</code></pre>"},{"location":"#kv-cache-optimized-gpt-2","title":"KV Cache Optimized GPT-2","text":"<pre><code>python toyllm/cli/run_gpt2_kv.py --help  # View available options\npython toyllm/cli/run_gpt2_kv.py         # Run with default settings\n</code></pre>"},{"location":"#speculative-sampling_1","title":"Speculative Sampling","text":"<pre><code>python toyllm/cli/run_speculative_sampling.py --help  # View available options\npython toyllm/cli/run_speculative_sampling.py         # Run with default settings\n</code></pre>"},{"location":"#benchmarking","title":"Benchmarking","text":"<pre><code>python toyllm/cli/benchmark/bench_gpt2kv.py --help  # View available options\npython toyllm/cli/benchmark/bench_gpt2kv.py         # Run benchmarks\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>toyllm/\n\u251c\u2500\u2500 cli/                    # Command-line interface scripts\n\u251c\u2500\u2500 gpt2/                   # GPT-2 specific implementations\n\u251c\u2500\u2500 gpt2_kv/                # KV-cache optimized GPT-2\n\u251c\u2500\u2500 sps/                    # Speculative sampling implementations\n\u251c\u2500\u2500 util/                   # Utility functions\n\u2514\u2500\u2500 models/                 # Model weights and configurations\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project is inspired by and builds upon the following excellent resources:</p> <ul> <li>rasbt/LLMs-from-scratch</li> <li>neelnanda-io/TransformerLens</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Refactor the code to enhance readability and maintainability</li> </ul>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Speculative Sampling</li> </ul>"},{"location":"changelog/#internal","title":"Internal","text":"<ul> <li>Use strict ruff linting rules</li> </ul>"},{"location":"changelog/#010-2024-05-20","title":"[0.1.0] - 2024-05-20","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>GPT-2 model</li> </ul>"},{"location":"blog/","title":"Blog","text":""}]}